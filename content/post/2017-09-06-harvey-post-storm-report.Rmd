---
title: Hurricane Harvey Post Storm Report
author: ~
date: '2017-09-06'
slug: hurricane-harvey-report
categories: [r, hurricanes, harvey]
tags: [r, hurricanes, harvey]
output:
  blogdown::html_page:
    toc: true
    fig_align: center
---

On August 26, 2017, Hurricane Harvey made landfall along the central Texas gulf coast as a category four hurricane. The storm weakened but remained stationary in southeast Texas spawing dozens of tornadoes and dropping over 30 inches of rain in many locations with isolated amounts of over 50 inches.

Harvey was the first major hurricane to strike the Texas coast since Hurricane Rita in 2005; the first category four hurricane to make landfall since Hurricane Bret in 1999.

In early September, National Weather Service (NWS) offices in Brownsville, Corpus Christi, San Antonio, and Houston, Texas, and Lake Charles, Louisiana released preliminary data reports on Hurricane Harvey. The following data has been extracted from those reports.

```{r libraries, echo = FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(ggrepel)
library(knitr)
library(lubridate)
library(pander)
library(purrr)
library(rrricanes)
library(rrricanesdata)
library(stringr)
library(tibble)
library(tidyr)
```

```{r load-data, echo = FALSE}
# URLs to reports. These links point to the latest product so the text may 
# change over time. Raw data will be saved in the GitHub repo:
# https://github.com/timtrice/web

if (!(file.exists("data/harvey-post-storm-report.rds"))) {
  rpts <- c(
    "bro" = "ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.kbro.psh.bro.txt", 
    "crp" = "ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.kcrp.psh.crp.txt", 
    "ewx" = "ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.kewx.psh.ewx.txt", 
    "hgx" = "ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.khgx.psh.hgx.txt", 
    "lch" = "ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.klch.psh.lch.txt", 
    "lix" = "ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.klix.psh.lix.txt")
  
  # Read data
  txt <- map(rpts, ~readLines(.x))
  
  # Save data
  saveRDS(txt, file = file.path("data/harvey-post-storm-report.rds"))
  message("ok")
} else {
  txt <- readRDS(file = file.path("data/harvey-post-storm-report.rds"))
}
```

```{r slp, echo = FALSE, warning = FALSE}
## ---- Section A. Lowest Sea Level Pressure ----
## ---- Section B. Marine Obs ----
slp_raw <- c(map(txt, ~.[grep("^A\\.", .):grep("^C\\.", .)])) %>% 
  flatten_chr()

# Get a count of all rows that begin with a latitude followed by longitude. 
# This will tell me exactly how many records I have.
# * \\s between lat and lon may be one or multiple lengths. 
slp_obs_ptn <- "^\\d\\d\\.\\d\\d\\s*-*\\d\\d\\d*\\.\\d\\d.+"
slp_n <- sum(str_count(slp_raw, slp_obs_ptn))

# Get indices of values for n
slp_obs_n <- str_which(slp_raw, slp_obs_ptn)
# The location for the obs will be the line immediatley preceeding it. 
# Therefore, we can get the station data by calculating x - 1
slp_stations_n <- slp_obs_n - 1

# Load stations
# Here, I trim the strings then take the nchar of longest string, round to 
# nearest ten and pad the string. I'll use this to help extract data.
slp_stations <- slp_raw[slp_stations_n] %>% 
  str_trim() %>% 
  str_pad(width = round(max(nchar(.)), digits = -1), side = "right") %>% 
  # Replace first "-" with "\t" to help split ID and Station
  str_replace("\\s*-\\s*", "\t")

# Load observations and trim
slp_obs <- slp_raw[slp_obs_n]

# Combine stations and obs
slp <- str_c(slp_stations, slp_obs)

# Begin extraction. Move to dataframe and rename variables.
slp_df <- str_match(slp, 
          sprintf("^%s%s%s%s%s%s%s%s%s%s%s%s$", 
                  # ID-Station
                  "(\\w+)\t*(?<=\t{0,1})(.+)(?=\\s+\\d{2}\\.\\d{2})",
                  # Lat
                  "\\s+(\\d{2}\\.\\d{2})",
                  # Lon
                  "\\s+-*(\\d{2,6}\\.\\d{2})",
                  # Pres
                  "\\s+(\\d{1,4}\\.\\d{1})*",
                  # PresDTd, PresDThm
                  "\\s+(\\w{1,3}|N)*/*(\\w{1,4})*",
                  # PresRmks
                  "\\s+(I)*",
                  # WindDir, Wind
                  "\\s+(\\w{3})/(\\d{3})", 
                  # Wind DTd, WindDThm
                  "\\s+(\\d{2,3})*/*(\\d{3,4})*", 
                  # WindRmks
                  "\\s+(I)*", 
                  # GustDir, Gust
                  "\\s+(\\w{3})*/*(\\d{3})*", 
                  # GustDTd, GustDThm
                  "\\s+(\\d{2,3})*/*(\\d{3,4})*", 
                  # GustRmks
                  "\\s+(I)*")) %>% 
  as_data_frame()

names(slp_df) <- c("txt", "ID", "Station", "Lat", "Lon", "Pres", "PresDTd", 
                   "PresDThm", "PresRmks", "WindDir", "Wind", "WindDTd", 
                   "WindDThm", "WindRmks", "GustDir", "Gust", "GustDTd", 
                   "GustDThm", "GustRmks")

slp_df <- arrange(slp_df, ID)

# Begin clean-up
# Trim all values
slp_df <- mutate_all(slp_df, .funs = str_trim)

# Clean up Pres variables
slp_df$Pres[slp_df$Pres %in% c("0", "0.0", "9999.0")] <- NA
slp_df$PresDTd[slp_df$PresDTd %in% c("N", "MM", "99", "206")] <- NA
slp_df$PresDThm[slp_df$PresDThm %in% c("A", "9999")] <- NA

# Clean up Wind variables
slp_df$WindDir[slp_df$WindDir %in% c("999", "MMM")] <- NA
slp_df$Wind[slp_df$Wind %in% c("999")] <- NA
slp_df$WindDTd[slp_df$WindDTd %in% c("000", "99")] <- NA
slp_df$WindDThm[slp_df$WindDThm %in% c("9999")] <- NA

# Clean up Gust variables
slp_df$GustDir[slp_df$GustDir %in% c("MMM", "999")] <- NA
slp_df$Gust[slp_df$Gust %in% c("999")] <- NA
slp_df$GustDTd[slp_df$GustDTd %in% c("99")] <- NA
slp_df$GustDThm[slp_df$GustDThm %in% c("000", "9999")] <- NA

# Convert numeric vars (with exception of date/time vars)
# For all positive Lon values, make negative
slp_df <- slp_df %>% 
  mutate_at(.vars = vars(Lat:Pres, WindDir:Wind, GustDir:Gust), 
            .funs = as.numeric) %>% 
  mutate(Lon = if_else(Lon > 0, Lon * -1, Lon))

# Correct the Lon for KXPY. Google Maps puts Port Fourchon at 
# 29.1055584,-90.2119496. Current values are 29.12, -903202.00. 
# I'll modify -903202.00 to -90.32
slp_df$Lon[slp_df$ID == "KXPY" & slp_df$Lon == -903202.00] <- -90.32

# ID TXVC-4 has been inadvertently split because of the first hyphen (the ob 
# is only an ID, no Station). Correct.
slp_df$ID[slp_df$ID == "TXVC" & slp_df$Station == "4"] <- "TXVC-4"
slp_df$Station[slp_df$ID == "TXVC-4"] <- NA

# Combine date variables for Pres, Wind, Gust to one POSIXct date variable
# Since all events of the storm occurred in August I can supply year, month.
# Some values will generate failure to parse due to being NA or other 
# invalid date or time.
slp_df <- slp_df %>% 
  mutate(PresDT = ymd_hm(sprintf("2017-08-%s %s", PresDTd, PresDThm)), 
         WindDT = ymd_hm(sprintf("2017-08-%s %s", WindDTd, WindDThm)), 
         GustDT = ymd_hm(sprintf("2017-08-%s %s", GustDTd, GustDThm)))

slp_df <- slp_df %>% 
  select(ID:Pres, PresDT,PresRmks, Wind, WindDir, WindDT, WindRmks, Gust, 
         GustDir, GustDT, GustRmks)
```

```{r rain, echo = FALSE, warning = FALSE}
## ---- Section C. STORM TOTAL RAINFALL ----
rain_raw <- c(map(txt, ~.[grep("^C\\.", .):grep("^D\\.", .)]))  %>% 
  flatten_chr()

rain_obs_ptn <- "^\\d\\d\\.\\d\\d\\s+-*\\d*\\d\\d\\.\\d\\d.*$"

rain_n <- sum(str_count(rain_raw, rain_obs_ptn))

rain_obs_n <- str_which(rain_raw, rain_obs_ptn)

rain_stations_n <- rain_obs_n - 1

rain_stations <- rain_raw[rain_stations_n] %>% str_trim()
max_n <- max(nchar(rain_stations))
rain_stations <- str_pad(rain_stations, 
                         width = round(max_n, digits = -1), 
                         side = "right")

rain_obs <- rain_raw[rain_obs_n]

rain <- str_c(rain_stations, rain_obs)

rain_df <- str_match(rain,
                 pattern = sprintf("^%s%s\\s+%s\\s+%s\\s*%s\\s+%s\\s+%s$", 
                                   "(.{29})", 
                                   "(.{19})", 
                                   "(.{0,12})",
                                   "(\\d{1,2}\\.\\d{2})", 
                                   "(I)*", 
                                   "(\\d{1,2}\\.\\d{2})",
                                   "-*(\\d{2,3}\\.\\d{2})")) %>% 
  as_data_frame() %>% 
  rename(txt = V1, Location = V2, County = V3, Station = V4, Rain = V5, 
         RainRmks = V6, Lat = V7, Lon = V8) %>% 
  mutate_at(.vars = c("Rain", "Lat", "Lon"), .funs = as.numeric) %>% 
  mutate(Lon = if_else(Lon > 0, Lon * -1, Lon)) %>% 
  select(Location, County, Station, Lat, Lon, Rain, RainRmks)
```

```{r tors, echo = FALSE, warning = FALSE}
# F. Tornadoes

# x = position data
# y = lat, lon row
# z = observation details
tor_raw <- c(map(txt, ~.[grep("^F\\.", .):grep("^G\\.", .)]))  %>% 
  flatten_chr()

# This will cut out 4 tornado observations from KLIX.
tor_y_ptn <- "^\\d\\d\\.\\d\\d\\s+-*\\d*\\d\\d\\.\\d\\d.*$"

tor_n <- sum(str_count(tor_raw, tor_y_ptn))

tor_y_n <- str_which(tor_raw, tor_y_ptn)

tor_x_n <- tor_y_n - 1

tor_stations <- tor_raw[tor_x_n] %>% str_trim()
max_n <- max(nchar(tor_stations))
tor_stations <- str_pad(tor_stations, 
                         width = round(max_n, digits = -1), 
                         side = "right")

tor_obs <- tor_raw[tor_y_n]

# To extract details I identify all elements of tor_raw that contain only \\s 
# as this tends to pre/proceed detail data of the observation. Once I know 
# where the beginning delimiter is I then find the very next delimiter. With 
# both of these values I'll map through tor_raw and subset the pieces.

# indices of tor_raw containing \\s elements
tor_z_n <- str_which(tor_raw, "^\\s*$")

# t is the indexes of tor_z_n which represent the first \\s after tor_obs
t <- match(c(tor_y_n + 1), tor_z_n)

# What indices of tor_z_n mark the beginning delimiter \\s
t_a <- tor_z_n[t]
# and the end delimiter
t_b <- tor_z_n[t + 1]

tor_details <- map2(t_a, t_b, ~tor_raw[.x:.y]) %>% flatten_chr()

# Now, at this point I want all the details on one line and to get rid of 
# the extra stuff. So, basically, some more reorg and clean-up.
tor_details <- str_c(tor_details, collapse = "\n") %>% 
  str_replace_all("\n\n+", "\t") %>% 
  str_replace_all("\n", " ") %>% 
  str_split("\t") %>% 
  map(str_trim) %>% 
  flatten_chr()

tor <- str_c(tor_stations, tor_obs)

tor_df <- str_match(tor, sprintf("^%s%s%s\\s+%s\\s+%s\\s+%s$", 
                                 "(.{29})", 
                                 "(.{17})", 
                                 "(\\d{2})/(\\d{4})", 
                                 "(\\w{3})", 
                                 "\\s+(\\d{2}\\.\\d{2})",
                                 "\\s+(-\\d{2}\\.\\d{2})$")) %>% 
  as_data_frame() %>% 
  mutate(Date = ymd_hm(sprintf("2017-08-%s %s", V4, V5))) %>% 
  rename(Location = V2, County = V3, Scale = V6, Lat = V7, Lon = V8) %>% 
  mutate_at(.vars = c("Lat", "Lon"), .funs = as.numeric) %>% 
  select(Location, County, Lat, Lon, Date, Scale)

# Now add in tor_details
tor_df$Details <- tor_details
```

```{r base-plot, echo = FALSE, message = FALSE}
# Draw a base plot
bp <- al_tracking_chart(color = "black", fill = "white", size = 0.1, res = 50) +
  labs(x = "Lon", y = "Lat") + 
  theme(legend.position = "bottom", 
        legend.direction = "horizontal")
```

## Hurricane Harvey

```{r data_al092017, echo = FALSE}
# Group Harvey advisories into two stages
AL092017 <- adv %>% 
  filter(Key == "AL092017") %>% 
  mutate(Group = if_else(Adv <= 11, 1, 2), 
         Class = factor(saffir(Wind), 
                        levels = c("TD", "TS", "HU1", "HU2", "HU3", "HU4", 
                                   "HU5"), 
                        labels = c("Tropical Depression", "Tropical Storm", 
                                   "Category 1", "Category 2", "Category 3", 
                                   "Category 4", "Category 5")))
# Harvey advs in Gulf
AL09017_gulf <- filter(AL092017, Adv >= 12)

# Landfall
lf.AL092017 <- data.frame("Date" = c(ymd_hm("2017-08-26 06:00", tz = "UTC"), 
                                     ymd_hm("2017-08-30 09:00", tz = "UTC")), 
                          "Location" = c("Copano Bay, TX", 
                                         "Cameron, LA"), 
                          "Lat" = c(28.08, 29.77), 
                          "Lon" = c(-96.88, -93.37),
                          "Wind" = c(110, 45))
```

Advisories were initiated on Potential Tropical Cyclone Nine on  the morning of August 17. Shortly thereafter, the cyclone was upgraded to a tropical storm but would not see additional strengthening as it moved into the eastern Caribbean Sea.

By the evening of August 19, Harvey had degenerated from a cyclone to a tropical wave (no closed center of circulation) and the National Hurricane Center discontinued issuing advisories on the system.

The full track of Hurricane Harvey is shown in [Figure 1](#fig:plot-al092017). 

```{r plot-al092017, echo = FALSE, fig.cap = "Hurricane Harvey had two life cycles as a tropical cyclone; the first in the western Atlantic and eastern Caribbean, and the second in the Gulf of Mexico."}
bp + 
  geom_point(data = AL092017, 
             aes(x = Lon, y = Lat, color = Class)) + 
  geom_path(data = AL092017, 
            aes(x = Lon, y = Lat, color = Class, group = Group)) + 
  guides(color = guide_legend(title = NULL, nrow = 1)) + 
  scale_color_brewer(type = "qual", palette = "Set1", direction = -1) +
  # scale_color_viridis(discrete = TRUE) + 
  coord_equal(xlim = c(min(AL092017$Lon), max(AL092017$Lon)), 
              ylim = c(min(AL092017$Lat), max(AL092017$Lat))) + 
  labs(title = "Hurricane Harvey - Full Track", 
       subtitle = "Harvey's track from August 17 to August 31, 2017")
```

By midday August 23, the remnants of Harvey had regenerated back into a tropical depression while in the southern Gulf of Mexico. The NHC began reissuing advisories on the system. 

[Figure 2](#fig:plot-al092017-gulf) shows the track of Hurricane Harvey after regeneration in the Gulf of Mexico. Once the cyclone redeveloped, it quickly began strengthening as it moved north and north-northwest towards the central Texas coastline. [Figure 3](#fig:plot-al092017-pres) and [Figure 5](#fig:plot-al092017-wind) show how the pressure and wind of Harvey changed rapidly as the storm intensified.

```{r plot-al092017-gulf, echo = FALSE, fig.cap = "Hurricane Harvey track in the Gulf of Mexico where the system intensified quickly into a category four hurricane."}
bp + 
  geom_point(data = AL09017_gulf, aes(x = Lon, y = Lat, color = Class)) + 
  geom_path(data = AL09017_gulf, 
            aes(x = Lon, y = Lat, color = Class, group = Group)) + 
  guides(color = guide_legend(title = NULL, nrow = 1)) + 
  scale_color_brewer(type = "qual", palette = "Set1", direction = -1) +
  coord_equal(xlim = c(min(AL09017_gulf$Lon) - 3, max(AL09017_gulf$Lon) + 3), 
              ylim = c(min(AL09017_gulf$Lat), max(AL09017_gulf$Lat))) + 
  labs(title = "Hurricane Harvey - Gulf of Mexico", 
       subtitle = "Harvey's track from August 23 to August 31, 2017")
```

### Barometric Pressure

Barometric pressure is the lowest atmospheric pressure either estimated or recorded in the center of a tropical cyclone. Generally speaking, the lower the barometric pressure, the stronger the cyclone. 

[Figure 3](#fig:plot-al092017-pres) shows the barometric pressure observations over 6-hour intervals during the life cycle of the cyclone. The gap from August 21 to August 23 is due to the system temporarily losing cyclone characteristics in the Caribbean Sea. 

```{r plot-al092017-pres, echo = FALSE, fig.cap = "Hurricane Harvey central barometric pressure observations during the life cycle of the tropical cyclone."}
AL092017 %>% 
  ggplot(aes(x = Date, y = Pressure, group = Group)) + 
  geom_line() + 
  geom_point(aes(color = Class), size = 3) + 
  guides(color = guide_legend(title = NULL, nrow = 1)) + 
  scale_color_brewer(type = "qual", palette = "Set1", direction = -1) +
  theme_bw() + 
  theme(legend.position = "bottom", 
        legend.box = "vertical") +
  labs(title = "Hurricane Harvey Pressure Profile", 
       subtitle = "Central barometric pressure observations over 6-hour intervals", 
       y = "Pressure (mb)")
```

[Figure 4](#fig:plot-pres) shows the lowest pressure observations as reported by the respective local NWS offices. As expected, the lowest values were recorded just north of Corpus Christi, Texas near the Rockport area where some locations reported pressure in the 940's (mb). 
[Table 1](#tbl:tbl-pres) shows the five lowest pressure observations recorded.

```{r plot-pres, echo = FALSE, fig.cap = "Rockport, Texas recorded the lowest pressure observations. Many observations are incomplete due to weather stations losing power or equipment during the storm."}
bp + 
  geom_point(data = filter(slp_df, 
                           !is.na(Pres)) %>% 
               arrange(desc(Pres)), 
             aes(x = Lon, y = Lat, color = Pres), shape = 7) + 
  scale_color_gradientn(colors = terrain.colors(10), 
                        breaks = c(940, 975, 1010), 
                        limits = c(940, 1010)) + 
  coord_equal(xlim = c(min(slp_df$Lon, na.rm = TRUE), 
                       max(slp_df$Lon, na.rm = TRUE)), 
              ylim = c(min(slp_df$Lat, na.rm = TRUE), 
                       max(slp_df$Lat, na.rm = TRUE))) + 
  labs(title = "Sea Level Pressure - Hurricane Harvey", 
       subtitle = "Lowest observed barometric pressure observations during Hurricane Harvey.")
```

The minimum pressure observed was 941.8mb at RCPT in Rockport, TX. Official landfall occurred at 06:00 UTC on the morning of August 26, 2017; 2 1/2 hours after the RCPT observation. This is approximately the same reported minimum central pressure listed by the National Hurricane Center (NHC) in [Public Advisory 23A](http://www.nhc.noaa.gov/archive/2017/al09/al092017.public_a.023.shtml?) which suggests pressure values may have been lower as the storm made passed over the station.

```{r tbl-pres, echo = FALSE, results = "asis"}
slp_df %>% 
  top_n(-5L, Pres) %>% 
  select(-c(Wind:GustRmks)) %>% 
  arrange(Pres) %>% 
  kable(caption = "Five lowest pressure observations (mb).")
```

The "I" in `PresRmks` denotes the observation is incomplete. 

Many stations lost equipment during the height of the storm, leading to many incomplete data observations (at least 70).

### Wind

[Figure 5](#fig:plot-al092017-wind) shows the quick increase in wind during Harvey's track over the Gulf of Mexico. It strengthened from a tropical depression to a category four hurricane in 60 hours; not record-breaking but quite impressive nonetheless.

```{r plot-al092017-wind, echo = FALSE, fig.cap = "Harvey strengthened from a tropical depression to category four hurricane in just over two days once in the Gulf of Mexico."}
AL092017 %>% 
  ggplot(aes(x = Date, y = Wind)) + 
  geom_line(aes(group = Group)) + 
  geom_point(aes(color = Class), size = 3) + 
  guides(color = guide_legend(title = NULL, nrow = 1)) + 
  scale_color_brewer(type = "qual", palette = "Set1", direction = -1) +
  theme_bw() + 
  theme(legend.position = "bottom", 
        legend.box = "vertical") +
  labs(title = "Hurricane Harvey Wind Profile", 
       subtitle = "Maximum sustained wind speeds over 6-hour intervals",
       y = "Wind (kts)")
```

[Figure 6](#fig:plot-slp-df-wind) shows the plot of all maximum wind values during Hurricane Harvey. As with barometric pressure, the highest wind values were recorded where Harvey made landfall but drop significantly at nearby locations.

```{r plot-slp-df-wind, echo = FALSE, fig.cap = "Maximum sustained wind observations as reported by weather stations during Hurricane Harvey."}
bp + 
  geom_point(data = filter(slp_df, !is.na(Wind)) %>% arrange(Wind), 
             aes(x = Lon, y = Lat, color = Wind), shape = 9) + 
  scale_color_gradientn(colors = rev(terrain.colors(10))) + 
  coord_equal(xlim = c(min(slp_df$Lon, na.rm = TRUE), 
                       max(slp_df$Lon, na.rm = TRUE)), 
              ylim = c(min(slp_df$Lat, na.rm = TRUE), 
                       max(slp_df$Lat, na.rm = TRUE))) + 
  labs(title = "Hurricane Harvey Maximum Wind Observations", 
       subtitle = "Highest wind observations during Hurricane Harvey.")
```

The highest wind value was recorded at ANPT2 several hours prior to the center of the hurricane moving ashore. NWS CRP (Corpus Christi) reports, 

> ANPT2 SENTINEL STOPPED REPORTING AND MAY NOT HAVE RECORDED MAXIMUM EVENT VALUES

```{r tbl-wind, echo = FALSE, results = "asis"}
slp_df %>% 
  top_n(5L, Wind) %>% 
  select(ID, Station, Lat, Lon, Wind, WindDT) %>% 
  arrange(desc(Wind)) %>% 
  kable(caption = "Five highest wind observations (kts).")
```

In addition to ANPT2, RCPT2 (Rockport), MAXT2, RTAT2, MIST2 and MIRT2 all failed prior to the peak of the hurricane. XWLD failed when the pier housing the sensor was swept away.

```{r tbl-wind-select, echo = FALSE, results = "asis"}
slp_df %>% 
  select(-c(Pres:PresRmks, Gust:GustRmks)) %>% 
  filter(!is.na(Wind),
         ID %in% c("RCPT2", "RTAT2", "ANPT2", "XWLD", "MAXT2", "MIST2", 
                   "MIRT2")) %>% 
  arrange(desc(Wind)) %>% 
  kable(caption = "Select Wind observations.")
```

### Rain

The story of Hurricane Harvey will always center around the significant flooding that took place as the hurricane weakened and stalled in southeast Texas. Harvey dropped record rainfall amounts across broad areas of southeast Texas turning highways into rivers and neighborhoods into lakes. 

```{r plot-rain, echo = FALSE, fig.cap = "Several locations across southeast Texas recorded over 30\" of rain with many isolated amounts of 40-50 inches."}
bp + 
  geom_point(data = filter(rain_df, !is.na(Rain)) %>% arrange(Rain), 
             aes(x = Lon, y = Lat, color = Rain), shape = 2) + 
  scale_color_gradientn(colors = rev(terrain.colors(10))) + 
  coord_equal(xlim = c(min(slp_df$Lon, na.rm = TRUE), 
                       max(slp_df$Lon, na.rm = TRUE)), 
              ylim = c(min(slp_df$Lat, na.rm = TRUE), 
                       max(slp_df$Lat, na.rm = TRUE))) + 
  labs(title = "Hurricane Harvey Rainfall Observations", 
       subtitle = "Maximum recorded rainfall, in inches, during Hurricane Harvey.")
```

Though the news focus of the flooding was centered in the Houston area (Harris County), the largest rainfall amounts were recorded in Chambers, Brazoria, Liberty and Jefferson counties.

```{r tbl-rain, echo = FALSE, results = "asis"}
rain_df %>% 
  top_n(5L, Rain) %>% 
  arrange(desc(Rain)) %>% 
  select(-RainRmks) %>% 
  kable(caption = "Top 5 Rainfall Amounts")
```

The text reports contain a Section D for Inland Flooding. Unfortunately at this time only the San Antonion/Austin NWS office have provided more info; generally river flooding and low water crossings. 

### Tornadoes

There are 24 tornado observations for Hurricane Harvey. However, some observations may be for the same tornado. For example, there are two observations for a tornado in Fort Bend county with the same `Lat`, `Lat` and `Date` value; only `Details` is different. 

```{r plot-tors, echo = FALSE, fig.cap = "All tornadoes generated from Hurricane Harvey were relatively weak, as is typical of tropical cyclones."}
tor_df$Scale <- factor(tor_df$Scale, 
                       levels = c("EF0", "EF1", "EF2", "EF3", "EF4", "EF5"), 
                       labels = c("EF0", "EF1", "EF2", "EF3", "EF4", "EF5"))

bp + 
  geom_point(data = filter(tor_df, !is.na(Scale)), 
             aes(x = Lon, y = Lat, color = Scale, fill = Scale), shape = 25) + 
  coord_equal(xlim = c(min(tor_df$Lon, na.rm = TRUE), 
                       max(tor_df$Lon, na.rm = TRUE)), 
              ylim = c(min(tor_df$Lat, na.rm = TRUE), 
                       max(tor_df$Lat, na.rm = TRUE))) + 
  labs(title = "Hurricane Harvey Tornado Reports", 
       subtitle = "Confirmed tornado touchdowns generated from Hurricane Harvey.")
```

[Table 5](#tbl:tbl-tors) shows all tornado observations during the event from each of the NWS offices. [Table Six](#tab:efs) identifies wind range values for `Scale`.

```{r tbl-tors, echo = FALSE, results = "asis"}
tor_df %>% 
  kable(caption = "Tornado Remarks, Hurricane Harvey")
```

```{r efs, echo = FALSE, results = "asis"}
efs <- data.frame("Scale" = c("EF0", "EF1", "EF2", "EF3", "EF4", "EF5"), 
                  "WindMPH" = c("65-85", "86-110", "111-135", "136-165", 
                                "166-200", ">200"), 
                  "WindKTS" = c("55-74", "75-96", "97-117", "118-143", 
                                "144-174", ">175"))

kable(efs, caption = "Enhanced Fujita Scale")
```

## Code

These text products are listed under the header ACUS74. They can be found on the [National Weather Service FTP server](ftp://tgftp.nws.noaa.gov/data/raw/ac/).

The following reports were obtained:

  * [Brownsville, TX (BRO)](ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.kbro.psh.bro.txt)

  * [Corpus Christi, TX (CRP)](ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.kcrp.psh.crp.txt)

  * [Austin/San Antonion, TX (EWX)](ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.kewx.psh.ewx.txt)

  * [Houston, TX (HGX)](ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.khgx.psh.hgx.txt)

  * [Lake Charles, LA (LCH)](ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.klch.psh.lch.txt)

  * [New Orleans, LA (LIX)](ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.klix.psh.lix.txt)

The links above point to the latest ACUS74 product issued by the respective NWS office. Therefore, it is possible that by the time you have read this article, the content of the text product has changed. Because of this, the rds data files have been saved to this website's [GitHub repository](https://github.com/timtrice/web/tree/harvey-report/content/post/data).

All times reported are in UTC. Pressure observations are in millibars. Wind and Gust observations are in knots.

### Libraries

The following libraries were used in this article:

```{r ref.label = "libraries", eval = FALSE}
```

### Data

To load the data, I just put the links into a named list. I originally had considered keeping the data separate by NWS office but determined this was not necessary. 

The code below looks for the data file mentioned earlier and, if it exists, loads the `txt` vector. Otherwise, each of the text products will be collected for parsing.

```{r ref.label = "load_data", eval = FALSE}
```

Each text product contains several sections:

  * Section A. Lowest Sea Level Pressure/Maximum Sustained Winds and Peak Gusts
  
    + Non-METAR Observations
  
  * Section B. Marine Observations
  
  * Section C. Storm Total Rainfall
  
  * Section D. Inland Flooding (not collected)
  
  * Section E. Maximum Storm Surge and Storm Tide (not collected)
  
  * Section F. Tornadoes
  
  * Section G. Storm Impacts by County (not collected)

Not all sections will contain data. For this article, all data collected was that which contained a latitude and longitude position.

Each section contains a data header:

```
A. LOWEST SEA LEVEL PRESSURE/MAXIMUM SUSTAINED WINDS AND PEAK GUSTS
---------------------------------------------------------------------
METAR OBSERVATIONS...
NOTE: ANEMOMETER HEIGHT IS 10 METERS AND WIND AVERAGING IS 2 MINUTES
---------------------------------------------------------------------
LOCATION  ID    MIN    DATE/     MAX      DATE/     PEAK    DATE/
LAT  LON        PRES   TIME      SUST     TIME      GUST    TIME
DEG DECIMAL     (MB)   (UTC)     (KT)     (UTC)     (KT)    (UTC)
---------------------------------------------------------------------
```

```
B. MARINE OBSERVATIONS...
NOTE: ANEMOMETER HEIGHT IN METERS AND WIND AVERAGING PERIOD IN
MINUTES INDICATED UNDER MAXIMUM SUSTAINED WIND IF KNOWN
---------------------------------------------------------------------
LOCATION  ID    MIN    DATE/     MAX      DATE/     PEAK    DATE/
LAT  LON        PRES   TIME      SUST     TIME      GUST    TIME
DEG DECIMAL     (MB)   (UTC)     (KT)     (UTC)     (KT)    (UTC)
---------------------------------------------------------------------
```

Additionally, a subsection of A exists:

```
NON-METAR OBSERVATIONS... 
NOTE: ANEMOMETER HEIGHT IN METERS AND WIND AVERAGING PERIOD IN
MINUTES INDICATED UNDER MAXIMUM SUSTAINED WIND IF KNOWN
---------------------------------------------------------------------
LOCATION  ID    MIN    DATE/     MAX      DATE/     PEAK    DATE/
LAT  LON        PRES   TIME      SUST     TIME      GUST    TIME
DEG DECIMAL     (MB)   (UTC)     (KT)     (UTC)     (KT)    (UTC)
---------------------------------------------------------------------
```

This subsection was added in as part of Sectoin A but is indistinguishable in the parsed dataset.

Observation examples are inluded in the relevant section below.

Numerous observations contain remarks (identified in the dataset by `ends_with("Rmks")`). Every section contains a Remarks footer; however, this may not be populated and, therefore, not every observation with a `.Rmks` variable would have additional Remarks listed in the text product. The additional Remarks were not collected.

The `.Rmks` legend is identified in the footer of each text product as:

  * "I" - Incomplete data
  
  * "E" - Estimated
  
All `.Rmks` variables in the datasets are either *NA* or "I".

Sections A and B may also contain additional anenometer height and wind-averaging period variables on a third line. This data was not collected but could easily have been; I did not feel it was relevant to this article. 

#### Sea Level Pressure and Marine Observations

A typical observation in Section A or B will look like the following:

```
RCPT2-ROCKPORT                                                      
28.02  -97.05   941.8 26/0336 I 017/059  26/0154 I 016/094 26/0148 I
```

There are 15 variables in the observation above (in order as they appear, with the example text):

  * `ID` (RCPT2)
  
  * `Station` (ROCKPORT)
  
  * `Lat` (28.02)
  
  * `Lon` (-97.05)
  
  * `Pres` (941.8) [barometric pressure, mb]
  
  * `PresDT` (26/0336) [date/time of `Pres` observation, UTC]
  
  * `PresRmks` (I) [incomplete pressure observation]
  
  * `Wind`, `WindDir` (017/059) [wind speed, kts, and wind direction]
  
  * `WindDT` (26/0154) [date/time of preceeding wind observation, UTC]
  
  * `WindRmks` (I) [incomplete wind observation]
  
  * `Gust`, `GustDir` (016/094) [maximum gust, kts, and direction]
  
  * `GustDT` (26/0148) [date/time of preceeding gust observation, UTC]
  
  * `GustRmks` (I) [incomplete gust observation]

Every observation has an empty line before and after which can be used as a delimiter to spilt observations.

But, first, I need to extract the relevant sections. To do this, I loop through the text products (`txt`) and identify where Section A and Section C begins. With these numerical indices, I can extract both sections, assigning the subset to `slp_raw`. 

```{r ref.label = "slp", echo = 3:4, eval = FALSE}
```

From there, I identify all vector elements that begin with a latitude and longitude field. I counted these values (`slp_n`) so that I know exactly how long my final results will be (and to check progress as I move along, making sure I haven't inadvertently removed anything).

Once I know where the observation indices are (`slp_obs_n`) I can find the station identification by calculating `slp_obs_n - 1`; this gives me `slp_stations_n`. 

```{r ref.label = "slp", echo = c(9:10, 13, 16), eval = FALSE}
```

Before merging the two vectors, I found some station values were not all the same length; I felt this would be beneficial with the regex. I create `slp_stations`, first trimming all values then finding the max length value. With the max length, I rounded up to the nearest ten and padded all all values to the right.

The last bit of manipulation involved replacing the first "-", if available, with a "\\t" character. This also helped me make it easier to split variables `ID` and `Station` since `Station` would contain additional "-" characters.

```{r ref.label = "slp", echo = 21:25, eval = FALSE}
```

Note that the code above split `ID` "TXCV-4" which would later be corrected.

Finally, I subset `slp_obs` and then with `slp_stations` make vector `slp`.

```{r ref.label = "slp", echo = c(28,31), eval = FALSE}
```

Following is a look at the head of `slp`:

```{r}
head(slp, n = 5L)
```

##### ID [`ID`], Station [`Station`] (opt)

Matching `ID` and `Station` is easier after switching the first "-" with a "\\t". `Station` is optional. To find the end of the string I simply looked for the latitude pattern that would follow. 

```{r ref.label = "slp", echo = c(36,37), eval = FALSE}
```

In some cases, for `Station`, old data seems to exist from previous instances of this product; particularly, "XDUL". Notice the text that seems to reference "Tropical Storm Cindy":

```{r}
slp[grep("^XDUL", slp)]
```

For that record, I left as-is.

##### Latitude [`Lat`]

Latitude was also very easy to extract. The pattern just looks for four digits with a decimal splitting in half.

```{r ref.label = "slp", echo = c(38, 39), eval = FALSE}
```

##### Longitude [`Lon`]

Extracting Longitude was a little bit more of a challenge. Most observations have a negative longitude value (since occurring in the northwestern hemisphere). This was accurately reflected in the text products; mostly. Some values did not contain the leading "-" such as `Station` "KEFD":

```{r}
slp[grep("^KEFD", slp)][2]
```

Additionally, "KXPY" just had a bad value:

```{r}
slp[grep("^KXPY", slp)]
```

"KXPY" would later be corrected manually using my best guess.

```{r ref.label = "slp", echo = c(97:100), eval = FALSE}
```

To accomodate the possibilities, I had to be loose with the number of digits expected in addition to making the negative sign optional.

```{r ref.label = "slp", echo = c(40, 41), eval = FALSE}
```

##### Minimum Pressure [`Pres`] *opt*

Expected pressure values would have a format like `\\d{3,4}\\.\\d{2}`. This would not be the case as there were many "0.0" values such as "FADT2":

```{r}
slp[grep("^FADT2", slp)]
```

Some observations even had values of "9999.0" which clearly were invalid (expected ranges were roughly between 940 and 1010). These would later be cleaned up.

```{r ref.label = "slp", echo = c(42, 43), eval = FALSE}
```

##### Date/time of pressure observation [`PresDT`] *opt*

`PresDT` was initially split to extract the date value first (`PresDTd`) followed by the "%h%m" value (`PresDThm`). The general format, "\\d{2}/\\d{4}" would not work primarily because many observations contained the text "MM" or "N/A". 

Additionally, some observations also held the value "99/9999". This would be accepted by the default format but would fail when converting the values to a valid date/time variable. These would later be cleaned. 

With this, I ended up being very generous with the regex.

```{r ref.label = "slp", echo = c(44, 45), eval = FALSE}
```

##### Pressure remarks [`PresRmks`] *opt*

As noted previously, some `Pres` variables may be incomplete for unknown reasons. These values would be indicated with the letter "I" as noted in the "RCPT2" example earlier.

```{r ref.label = "slp", echo = c(46, 47), eval = FALSE}
```

##### Maximum sustained wind direction [`WindDir`], Maximum sustained winds (opt) [`Wind`]

Variables `WindDir` and `Wind` were split with a "/" character. However, again, not all values were numeric as expected, such as "LOPL1":

```{r}
slp[grep("^LOPL1", slp)]
```

Invalid values such as "999" would later be marked as `NA`.

```{r ref.label = "slp", echo = c(48, 49), eval = FALSE}
```

The remaining variables followed generally the same rules as similar variables above (i.e., `GustDir` for `WindDir`, `GustDT` for `WindDT`, etc.). The final `str_match` call brought all expected observations in order.

```{r ref.label = "slp", echo = c(33:60), eval = FALSE}
```

#### Rainfall

Section C of the text products listed recorded rainfall observations across the Texas and Louisiana area. These observations were similar in format to those of pressure:

```
COLETO CREEK                 GOLIAD              CKDT2         9.42 I
28.73  -97.17
```

The following fields were extracted:

  * `Location` ("COLETO CREEK")
  
  * `Count` ("GOLIAD")
  
  * `Station` ("CKDT2") *opt*
  
  * `Rain` ("9.42")
  
  * `RainRmks` ("I")
  
  * `Lat` ("28.73")
  
  * `Lon` ("-97.17")

Extracting these observations followed the same premise for that of `slp`; identifying the latitude lines then subsetting those indices and the previous indices and combining into a vector.

Unlike `slp`, there were no surprises in cleaning this data. The regex used:

```{r ref.label = "rain", echo = c(23:36), eval = FALSE}
```

#### Tornadoes

Section F lists reported tornadoes for each region of responsibility. Some NWS offices reported no tornado observations. Others, such as Houston, reported at least a dozen.

An example observation is as follows:

```
4 NNE SEADRIFT               CALHOUN          25/2114          EF0   
28.43  -96.67

FACEBOOK PHOTOS AND VIDEO SHOWED A BRIEF TORNADO TOUCHED DOWN ON
GATES ROAD NEAR SEADRIFT. A SHED AND CARPORT WERE DESTROYED AND A
FEW TREES WERE BLOWN DOWN. RATED EF0. 
```

Each observation, again, was preceeded and proceeded by an empty line.

  * `Location` ("4 NNE SEADRIFT")
  
  * `County` ("CALHOUN")
  
  * `Date` ("25/2114")
  
  * `Scale` ("EFO")
  
  * `Lat` ("28.43")
  
  * `Lon` ("-96.67")
  
  * `Details` ("FACEBOOK PHOTOS...")

Extracting the first two lines used the same technique as previous. However, extracting the `Details` required a little creativity since many spread over several lines.

To do this, I took the values of `tor_y_n` which marked the indices of the latitude/longitude positions. Then, I created vector `tor_z_n` to identify all "^\\s+" elements in the original vector, `tor_raw`. With this, I was able to identify the first "^\\s+" index following the latitude/longitude line, `t_a` and then take the very next index, `t_b`. 

Apologies for the non-descriptive names; I was lacking ingenuity.

With `t_a` and `t_b`, I used `map2` through `tor_raw` to extract each subset. From that point there was some cleaning to bring the lines together as needed. If you have any better (even if slower, but more creative), I would love to hear them! I may have tried to get too creative with this task.

The regex was very similar to the `rain` regex as most items were evenly delimited.

## Previous Versions

  * [1](https://github.com/timtrice/web/blob/15f552b061a7200647cc2e7bfd8174fec631d816/content/post/2017-09-06-hurricane-harvey-post-storm-report-houston.Rmd)

## Session Info

```{r session-info}
pander::pander(sessionInfo())
```

```{r}
warnings()
```

