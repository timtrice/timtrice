---
title: Hurricane Harvey Post Storm Report
author: ~
date: '2017-09-06'
slug: hurricane-harvey-report
categories: [r, hurricanes, harvey]
tags: [r, hurricanes, harvey]
output:
  blogdown::html_page:
    toc: true
---

As of this publishing, National Weather Service (NWS) offices in Brownsville, Corpus Christi, San Antonio, and Houston, Texas, and Lake Charles, Louisiana released preliminary data reports on Hurricane Harvey.

These text products are listed under the header ACUS74. They can be found on the [National Weather Service FTP server](ftp://tgftp.nws.noaa.gov/data/raw/ac/).

The following reports were obtained:

  * [Brownsville, TX (BRO)](ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.kbro.psh.bro.txt)

  * [Corpus Christi, TX (CRP)](ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.kcrp.psh.crp.txt)

  * [Austin/San Antonion, TX (EWX)](ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.kewx.psh.ewx.txt)

  * [Houston, TX (HGX)](ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.khgx.psh.hgx.txt)

  * [Lake Charles, LA (LCH)](ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.klch.psh.lch.txt)

  * [New Orleans, LA (LIX)](ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.klix.psh.lix.txt)

The links above point to the latest ACUS74 product issued by the respective NWS office. Therefore, it is possible that by the time you have read this the content of the text product has changed. Because of this, the rds data files have been saved to this website's [GitHub repository](https://github.com/timtrice/web/tree/harvey-report/content/post/data).

```{r libraries, echo = FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(ggrepel)
library(knitr)
library(lubridate)
library(purrr)
library(rrricanes)
library(stringr)
library(tibble)
library(tidyr)
```

```{r load_data, echo = FALSE}
# URLs to reports. These links point to the latest product so the text may 
# change over time. Raw data will be saved in the GitHub repo:
# https://github.com/timtrice/web

if (!(file.exists("data/harvey-post-storm-report.rds"))) {
  rpts <- c(
    "bro" = "ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.kbro.psh.bro.txt", 
    "crp" = "ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.kcrp.psh.crp.txt", 
    "ewx" = "ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.kewx.psh.ewx.txt", 
    "hgx" = "ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.khgx.psh.hgx.txt", 
    "lch" = "ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.klch.psh.lch.txt", 
    "lix" = "ftp://tgftp.nws.noaa.gov/data/raw/ac/acus74.klix.psh.lix.txt")
  
  # Read data
  txt <- map(rpts, ~readLines(.x))
  
  # Save data
  saveRDS(txt, file = file.path("data/harvey-post-storm-report.rds"))
  message("ok")
} else {
  txt <- readRDS(file = file.path("data/harvey-post-storm-report.rds"))
}
```

```{r slp, echo = FALSE, warning = FALSE}
## ---- Section A. Lowest Sea Level Pressure ----
## ---- Section B. Marine Obs ----
slp_raw <- c(map(txt, ~.[grep("^A\\.", .):grep("^C\\.", .)])) %>% 
  flatten_chr()

# Get a count of all rows that begin with a latitude followed by longitude. 
# This will tell me exactly how many records I have.
# * \\s between lat and lon may be one or multiple lengths. 
slp_obs_ptn <- "^\\d\\d\\.\\d\\d\\s*-*\\d\\d\\d*\\.\\d\\d.+"
slp_n <- sum(str_count(slp_raw, slp_obs_ptn))

# Get indices of values for n
slp_obs_n <- str_which(slp_raw, slp_obs_ptn)
# The location for the obs will be the line immediatley preceeding it. 
# Therefore, we can get the station data by calculating x - 1
slp_stations_n <- slp_obs_n - 1

# Load stations
# Here, I trim the strings then take the nchar of longest string, round to 
# nearest ten and pad the string. I'll use this to help extract data.
slp_stations <- slp_raw[slp_stations_n] %>% 
  str_trim() %>% 
  str_pad(width = round(max(nchar(.)), digits = -1), side = "right") %>% 
  # Replace first "-" with "\t" to help split ID and Station
  str_replace("\\s*-\\s*", "\t")

# Load observations and trim
slp_obs <- slp_raw[slp_obs_n]

# Combine stations and obs
slp <- str_c(slp_stations, slp_obs)

# Begin extraction. Move to dataframe and rename variables.
slp_df <- str_match(slp, 
          sprintf("^%s%s%s%s%s%s%s%s%s%s%s%s$", 
                  # ID-Station
                  "(\\w+)\t*(?<=\t{0,1})(.+)(?=\\s+\\d{2}\\.\\d{2})",
                  # Lat
                  "\\s+(\\d{2}\\.\\d{2})",
                  # Lon
                  "\\s+-*(\\d{2,6}\\.\\d{2})",
                  # Pres
                  "\\s+(\\d{1,4}\\.\\d{1})*",
                  # PresDTd, PresDThm
                  "\\s+(\\w{1,3}|N)*/*(\\w{1,4})*",
                  # PresRmks
                  "\\s+(I)*",
                  # WindDir, Wind
                  "\\s+(\\w{3})/(\\d{3})", 
                  # Wind DTd, WindDThm
                  "\\s+(\\d{2,3})*/*(\\d{3,4})*", 
                  # WindRmks
                  "\\s+(I)*", 
                  # GustDir, Gust
                  "\\s+(\\w{3})*/*(\\d{3})*", 
                  # GustDTd, GustDThm
                  "\\s+(\\d{2,3})*/*(\\d{3,4})*", 
                  # GustRmks
                  "\\s+(I)*")) %>% 
  as_data_frame() %>% 
  rename(txt = V1, ID = V2, Station = V3, Lat = V4, Lon = V5, Pres = V6, 
         PresDTd = V7, PresDThm = V8, PresRmks = V9, WindDir = V10, 
         Wind = V11, WindDTd = V12, WindDThm = V13, WindRmks = V14, 
         GustDir = V15, Gust = V16, GustDTd = V17, GustDThm = V18,
         GustRmks = V19) %>% 
  arrange(ID)

# Begin clean-up

# Trim all values
slp_df <- mutate_all(slp_df, .funs = str_trim)

# Clean up Pres variables
slp_df$Pres[slp_df$Pres %in% c("0", "0.0", "9999.0")] <- NA
slp_df$PresDTd[slp_df$PresDTd %in% c("N", "MM", "99", "206")] <- NA
slp_df$PresDThm[slp_df$PresDThm %in% c("A", "9999")] <- NA

# Clean up Wind variables
slp_df$WindDir[slp_df$WindDir %in% c("999", "MMM")] <- NA
slp_df$Wind[slp_df$Wind %in% c("999")] <- NA
slp_df$WindDTd[slp_df$WindDTd %in% c("000", "99")] <- NA
slp_df$WindDThm[slp_df$WindDThm %in% c("9999")] <- NA

# Clean up Gust variables
slp_df$GustDir[slp_df$GustDir %in% c("MMM", "999")] <- NA
slp_df$Gust[slp_df$Gust %in% c("999")] <- NA
slp_df$GustDTd[slp_df$GustDTd %in% c("99")] <- NA
slp_df$GustDThm[slp_df$GustDThm %in% c("000", "9999")] <- NA

# Convert numeric vars (with exception of date/time vars)
# For all positive Lon values, make negative
slp_df <- slp_df %>% 
  mutate_at(.vars = vars(Lat:Pres, WindDir:Wind, GustDir:Gust), 
            .funs = as.numeric) %>% 
  mutate(Lon = if_else(Lon > 0, Lon * -1, Lon))

# Correct the Lon for KXPY. Google Maps puts Port Fourchon at 
# 29.1055584,-90.2119496. Current values are 29.12, -903202.00. 
# I'll modify -903202.00 to -90.32
slp_df$Lon[slp_df$ID == "KXPY" & slp_df$Lon == -903202.00] <- -90.32

# ID TXVC-4 has been inadvertently split because of the first hyphen (the ob 
# is only an ID, no Station). Correct.
slp_df$ID[slp_df$ID == "TXVC" & slp_df$Station == "4"] <- "TXVC-4"
slp_df$Station[slp_df$ID == "TXVC-4"] <- NA

# Combine date variables for Pres, Wind, Gust to one POSIXct date variable
# Since all events of the storm occurred in August I can supply year, month.
# Some values will generate failure to parse due to being NA or other 
# invalid date or time.
slp_df <- slp_df %>% 
  mutate(PresDT = ymd_hm(sprintf("2017-08-%s %s", PresDTd, PresDThm)), 
         WindDT = ymd_hm(sprintf("2017-08-%s %s", WindDTd, WindDThm)), 
         GustDT = ymd_hm(sprintf("2017-08-%s %s", GustDTd, GustDThm)))

slp_df <- slp_df %>% 
  select(ID:Pres, PresDT, Wind, WindDir, WindDT, WindRmks, Gust, GustDir, 
         GustDT, GustRmks)
```

```{r rain, echo = FALSE, warning = FALSE}
## ---- Section C. STORM TOTAL RAINFALL ----
rain_raw <- c(map(txt, ~.[grep("^C\\.", .):grep("^D\\.", .)]))  %>% 
  flatten_chr()

rain_obs_ptn <- "^\\d\\d\\.\\d\\d\\s+-*\\d*\\d\\d\\.\\d\\d.*$"

rain_n <- sum(str_count(rain_raw, rain_obs_ptn))

rain_obs_n <- str_which(rain_raw, rain_obs_ptn)

rain_stations_n <- rain_obs_n - 1

rain_stations <- rain_raw[rain_stations_n] %>% str_trim()
max_n <- max(nchar(rain_stations))
rain_stations <- str_pad(rain_stations, 
                         width = round(max_n, digits = -1), 
                         side = "right")

rain_obs <- rain_raw[rain_obs_n]

rain <- str_c(rain_stations, rain_obs)

rain_df <- str_match(rain,
                 pattern = sprintf("^%s%s\\s+%s\\s+%s\\s*%s\\s+%s\\s+%s$", 
                                   "(.{29})", 
                                   "(.{19})", 
                                   "(.{0,12})",
                                   "(\\d{1,2}\\.\\d{2})", 
                                   "(I)*", 
                                   "(\\d{1,2}\\.\\d{2})",
                                   "-*(\\d{2,3}\\.\\d{2})")) %>% 
  as_data_frame() %>% 
  rename(txt = V1, Location = V2, County = V3, Station = V4, Rain = V5, 
         RainRmks = V6, Lat = V7, Lon = V8) %>% 
  mutate_at(.vars = c("Rain", "Lat", "Lon"), .funs = as.numeric) %>% 
  mutate(Lon = if_else(Lon > 0, Lon * -1, Lon))
```

```{r tors, echo = FALSE, warning = FALSE}
# F. Tornadoes

# x = position data
# y = lat, lon row
# z = observation details
tor_raw <- c(map(txt, ~.[grep("^F\\.", .):grep("^G\\.", .)]))  %>% 
  flatten_chr()

# This will cut out 4 tornado observations from KLIX.
tor_y_ptn <- "^\\d\\d\\.\\d\\d\\s+-*\\d*\\d\\d\\.\\d\\d.*$"

tor_n <- sum(str_count(tor_raw, tor_y_ptn))

tor_y_n <- str_which(tor_raw, tor_y_ptn)

tor_x_n <- tor_y_n - 1

tor_stations <- tor_raw[tor_x_n] %>% str_trim()
max_n <- max(nchar(tor_stations))
tor_stations <- str_pad(tor_stations, 
                         width = round(max_n, digits = -1), 
                         side = "right")

tor_obs <- tor_raw[tor_y_n]

# To extract details I identify all elements of tor_raw that contain only \\s 
# as this tends to pre/proceed detail data of the observation. Once I know 
# where the beginning delimiter is I then find the very next delimiter. With 
# both of these values I'll map through tor_raw and subset the pieces.

# indices of tor_raw containing \\s elements
tor_z_n <- str_which(tor_raw, "^\\s*$")

# t is the indexes of tor_z_n which represent the first \\s after tor_obs
t <- match(c(tor_y_n + 1), tor_z_n)

# What indices of tor_z_n mark the beginning delimiter \\s
t_a <- tor_z_n[t]
# and the end delimiter
t_b <- tor_z_n[t + 1]

tor_details <- map2(t_a, t_b, ~tor_raw[.x:.y]) %>% flatten_chr()

# Now, at this point I want all the details on one line and to get rid of 
# the extra stuff. So, basically, some more reorg and clean-up.
tor_details <- str_c(tor_details, collapse = "\n") %>% 
  str_replace_all("\n\n+", "\t") %>% 
  str_replace_all("\n", " ") %>% 
  str_split("\t") %>% 
  map(str_trim) %>% 
  flatten_chr()

tor <- str_c(tor_stations, tor_obs)

tor_df <- str_match(tor, sprintf("^%s%s%s\\s+%s\\s+%s\\s+%s$", 
                                 "(.{29})", 
                                 "(.{17})", 
                                 "(\\d{2})/(\\d{4})", 
                                 "(\\w{3})", 
                                 "\\s+(\\d{2}\\.\\d{2})",
                                 "\\s+(-\\d{2}\\.\\d{2})$")) %>% 
  as_data_frame() %>% 
  mutate(Date = ymd_hm(sprintf("2017-08-%s %s", V4, V5))) %>% 
  rename(Location = V2, County = V3, Scale = V6, Lat = V7, Lon = V8) %>% 
  mutate_at(.vars = c("Lat", "Lon"), .funs = as.numeric) %>% 
  select(Location, County, Lat, Lon, Date, Scale)

# Now add in tor_details
tor_df$Details <- tor_details
```

```{r base_plot, echo = FALSE, message = FALSE}
# Draw a base plot
bp <- al_tracking_chart(color = "black", fill = "white", size = 0.1, res = 50) +
  labs(x = "Lon", y = "Lat") + 
  theme(legend.position = "bottom", 
        legend.direction = "horizontal")
```

## Plots and Charts

### Barometric Pressure

```{r plot_pres, echo = FALSE}
bp + 
  geom_point(data = filter(slp_df, !is.na(Pres)) %>% arrange(desc(Pres)), 
             aes(x = Lon, y = Lat, color = Pres), shape = 7) + 
  scale_colour_gradientn(colours = terrain.colors(10)) + 
  coord_equal(xlim = c(min(slp_df$Lon, na.rm = TRUE), 
                       max(slp_df$Lon, na.rm = TRUE)), 
              ylim = c(min(slp_df$Lat, na.rm = TRUE), 
                       max(slp_df$Lat, na.rm = TRUE))) + 
  ggtitle("Hurricane Harvey Pressure Observations")
```

### Wind

```{r plot_wind, echo = FALSE}
bp + 
  geom_point(data = filter(slp_df, !is.na(Wind)) %>% arrange(Wind), 
             aes(x = Lon, y = Lat, color = Wind), shape = 9) + 
  scale_colour_gradientn(colours = rev(terrain.colors(10))) + 
  coord_equal(xlim = c(min(slp_df$Lon, na.rm = TRUE), 
                       max(slp_df$Lon, na.rm = TRUE)), 
              ylim = c(min(slp_df$Lat, na.rm = TRUE), 
                       max(slp_df$Lat, na.rm = TRUE))) + 
  ggtitle("Hurricane Harvey Maximum Wind Observations")
```

### Rain

```{r plot_rain, echo = FALSE}
bp + 
  geom_point(data = filter(rain_df, !is.na(Rain)) %>% arrange(Rain), 
             aes(x = Lon, y = Lat, color = Rain), shape = 2) + 
  scale_color_gradientn(colors = rev(terrain.colors(10))) + 
  coord_equal(xlim = c(min(slp_df$Lon, na.rm = TRUE), 
                       max(slp_df$Lon, na.rm = TRUE)), 
              ylim = c(min(slp_df$Lat, na.rm = TRUE), 
                       max(slp_df$Lat, na.rm = TRUE))) + 
  ggtitle("Hurricane Harvey Rainfall Observations")
```

### Tornadoes

```{r plot_tors, echo = FALSE}
bp + 
  geom_point(data = filter(tor_df, !is.na(Scale)), 
             aes(x = Lon, y = Lat, color = Scale, fill = Scale), shape = 25) + 
  coord_equal(xlim = c(min(tor_df$Lon, na.rm = TRUE), 
                       max(tor_df$Lon, na.rm = TRUE)), 
              ylim = c(min(tor_df$Lat, na.rm = TRUE), 
                       max(tor_df$Lat, na.rm = TRUE))) + 
  ggtitle("Hurricane Harvey Tornado Reports")
```

## Tables

### Barometric Pressure

```{r tbl_pres, echo = FALSE}
kable(slp_df %>% 
        select(ID, Station, Pres, PresDT) %>% 
        top_n(-10L, wt = Pres) %>% 
        arrange(Pres),
      caption = "Hurricane Harvey Barometric Pressure Observations (Top 10)")
```

## Code

Explain the why you did what you did

### Libraries

```{r ref.label = "libraries", eval = FALSE}
```

### Data

```{r ref.label = "data", eval = FALSE}
```

#### Sea Level Pressure and Marine Observations

Each section has a header:

```
A. LOWEST SEA LEVEL PRESSURE/MAXIMUM SUSTAINED WINDS AND PEAK GUSTS
---------------------------------------------------------------------
METAR OBSERVATIONS...
NOTE: ANEMOMETER HEIGHT IS 10 METERS AND WIND AVERAGING IS 2 MINUTES
---------------------------------------------------------------------
LOCATION  ID    MIN    DATE/     MAX      DATE/     PEAK    DATE/
LAT  LON        PRES   TIME      SUST     TIME      GUST    TIME
DEG DECIMAL     (MB)   (UTC)     (KT)     (UTC)     (KT)    (UTC)
---------------------------------------------------------------------
```

```
B. MARINE OBSERVATIONS...
NOTE: ANEMOMETER HEIGHT IN METERS AND WIND AVERAGING PERIOD IN
MINUTES INDICATED UNDER MAXIMUM SUSTAINED WIND IF KNOWN
---------------------------------------------------------------------
LOCATION  ID    MIN    DATE/     MAX      DATE/     PEAK    DATE/
LAT  LON        PRES   TIME      SUST     TIME      GUST    TIME
DEG DECIMAL     (MB)   (UTC)     (KT)     (UTC)     (KT)    (UTC)
---------------------------------------------------------------------
```

Additionally, a subsection of A exists:

```
NON-METAR OBSERVATIONS... 
NOTE: ANEMOMETER HEIGHT IN METERS AND WIND AVERAGING PERIOD IN
MINUTES INDICATED UNDER MAXIMUM SUSTAINED WIND IF KNOWN
---------------------------------------------------------------------
LOCATION  ID    MIN    DATE/     MAX      DATE/     PEAK    DATE/
LAT  LON        PRES   TIME      SUST     TIME      GUST    TIME
DEG DECIMAL     (MB)   (UTC)     (KT)     (UTC)     (KT)    (UTC)
---------------------------------------------------------------------
```

That is irrelevant and is ignored. 

I loop through the text products (`txt`) and extract both sections A and B by identifying where section C begins. This gives me a start and end index that I can then subset to `slp_raw`. 

```{r ref.label = "slp", echo = 3:4, eval = FALSE}
```

From there, I identify all values that begin with a latitude and longitude field; our observations. First I count these values (`slp_n`) so that I know exactly how long my final results will be (and to check progress as I move along, making sure I haven't inadvertently removed anything).

Once I know where the observation indices are (`slp_obs_n`) I can find the station identification by calculating `slp_obs_n - 1`; this gives me `slp_stations_n`. 

```{r ref.label = "slp", echo = c(9:10, 13, 16), eval = FALSE}
```

Before merging the two vectors, I found some station values were not all the same length, that this would be beneficial with the regex. I create `slp_stations`, first trimming all values then finding the max length value. With the max length, I rounded up to the nearest ten and padded all all values to a length of 60 characters (the max length was 56).

The last bit of tidying involved replacing the first "-", if available, with a "\\t" character. This also helped me make it easier to split variables `ID` and `Station` since `Station` would contain additional "-" characters.

```{r ref.label = "slp", echo = 21:25, eval = FALSE}
```

Finally, I subset `slp_obs` and then with `slp_stations` make vector `slp`.

```{r ref.label = "slp", echo = c(28,31), eval = FALSE}
```

Following is a look at the head of `slp`:

```{r}
head(slp, n = 5L)
```

Following are the expected fields that will be retrieved from the results of `slp`.

##### Station ID [`ID`]

1. Hyphen character does not necessarily separate `ID` and `Station`:

```{r}
slp[grep("^TXVC-4", slp)]
```

##### Station [`Station`] *opt*

1. Old data may exist; see this reference to Tropical Storm Cindy:

```{r}
slp[grep("^XDUL", slp)]
```

##### Latitude [`Lat`]

All values are clean. As entirety of event occured in northwestern hemisphere, values should be formatted as `\\d{2}\\.\\d{2}`.

##### Longitude [`Lon`]

All longitude values should be formatted as `-\\d{2,3}\\.\\d{2}`. 

1. Some values are not necessarily negative ("95.17" instead of "-95.17"):

```{r}
slp[grep("^KEFD", slp)][2]
```

2. Some values are just incorrect ("-903202.00"):

```{r}
slp[grep("^KXPY", slp)]
```

##### Minimum Pressure [`Pres`] *opt*

Expected format: `\\d{3,4}\\.\\d{2}`. Value is optional and may be `\\s{5,6}`.

1. Some values may be "0.0" (note the preceeding "N/A" which is also unusual in this dataset):

```{r}
slp[grep("^FADT2", slp)]
```

##### Date/time of pressure observation [`PresDT`] *opt*

If `Pres` is provided, the date/time of that observation is also recorded in `\\d{2}/\\d{4}` format. 

If `Pres` is empty (`\\s{5,6}`), `PresDT` will also be empty.

If `Pres` is "0.0" then `PresDT` contains a "MM" string:

```{r}
slp[grep("^SPLL1", slp)]
```

In some cases, `Pres` may be "9999.0" which is invalid. In these cases, `PresDT` is "/" or "99/9999" (note the optional preceeding "I"; `PresRmks`).

```{r}
slp[grep("^KPKV", slp)]
```

```{r}
slp[grep("^KTXSMILE2", slp)]
```

`PresDT` may also be "N/A":

```{r}
slp[grep("^FADT2", slp)]
```

##### Pressure remarks [`PresRmks`] *opt*

`PresRmks` may precede `PresDT` as stated above. Additionally, it may also exist for valid `Pres` and `PresDT` values:

```{r}
slp[grep("^KCWF", slp)]
```
##### Maximum sustained wind direction [`WindDir`]

Wind direction is formatted as `\\d{3}`. It is the first part of the Wind field string, separated from `Wind` with a "/" character.

1. Some values may be "999":

```{r}
slp[grep("^KTXS33", slp)]
```

2. Some values may also contain "MMM":

```{r}
slp[grep("^LOPL1", slp)]
```

##### Maximum sustained winds (opt) [`Wind`]

Precedes `WindDir` separated by a "/" character.

1. Valid values may be preceeded by invalid `WindDir` values:

```{r}
slp[grep("^FCMP", slp)]
```

It is assumed the observation above is reporting a 88kt wind but from an unrecorded direction.

##### Date/time of winds [`WindDT`]

Same as `PresDT`

##### Wind remarks (opt) [`WindRmks`]

Same as `PresRmks`

##### Peak gust direction [`GustDir`]

Same as `WindDir`

##### Peak Gusts [`Gust`]

Same as `WindDir`

##### Date/time of peak gusts [`GustDT`]

Same as `PresDT`

##### Gust remarks (opt) [`PresRmks`]

Same as `PresRmks`

```{r ref.label = "slp_df", eval = FALSE}
```

## Source

[Post Tropical Cyclone Report - Tropical Storm Harvey](https://nwschat.weather.gov/p.php?pid=201709062030-KHGX-ACUS74-PSHHGX)

## Previous Versions

  * [1](https://github.com/timtrice/web/blob/15f552b061a7200647cc2e7bfd8174fec631d816/content/post/2017-09-06-hurricane-harvey-post-storm-report-houston.Rmd)

## Session Info

```{r session_info}
pander::pander(sessionInfo())
```
