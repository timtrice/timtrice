---
layout: post
title: Cleaning Dates/Times in NCDC Storm Events
tags:
- NCDC_Storm_Events
excerpt: Cleaning dates and times and correcting timezones in the NCDC Storm Events datasets.
---

<!-- START doctoc -->
<!-- END doctoc -->

```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(fig.width = 10)
```

```{r, echo = FALSE, message = FALSE}
library(data.table)
library(ggplot2)
library(knitr)
library(lubridate)
library(maps)
library(NCDCStormEvents)
```

## Dates and Times

The date and time variables in this dataset are, honestly, quite horrible. In the `details` dataset alone there are 11 variables for date and time where we only need one. 

But we don't want to delete those variables or edit them. Instead, we want to create lookup tables. Remember, `EVENT_ID` serves as our key so we can tie in any dataset by `EVENT_ID` alone. 

With that, we can start a rather arduous process of cleaning the dates and times. First, we want to start with time zones. 

## Time Zones

In my exploratory processes with this dataset, working with the time zones has been a bit of a nightmare. Let's take a sample look:

```{r}
DT <- get_data(1965, "details")

unique(DT$CZ_TIMEZONE)

tz_to_dt(DT[, .(EVENT_ID, CZ_TIMEZONE)])
```

This doesn't seem too bad. Let's dig a bit deeper.

`MDT` has one row; let's check it out.

```{r}
tmp <- DT[MDT, .(EVENT_ID, CZ_NAME, STATE, CZ_TIMEZONE, BEGIN_LAT, 
                 BEGIN_LON)]

US <- map_data("state")

bp <- list(geom_polygon(data = US, aes(x = long, y = lat, group = group), 
                 colour = "grey10", fill = "white"), 
           geom_point(size = 2, colour = "red"))

ggplot(tmp, aes(x = BEGIN_LON, y = BEGIN_LAT)) + bp
```

Well, that doesn't seem right...

```{r}
tmp
```

So, our city - Brookings, SD is actually in the central time zone according to [TimeAndDate.com](http://www.timeanddate.com/worldclock/usa/sioux-falls). 

Maybe it's something in my code? Let's just pull straight from `DT`.

```{r}
tmp <- DT[CZ_TIMEZONE == "MDT", .(EVENT_ID, CZ_NAME, STATE, CZ_TIMEZONE, 
                                  BEGIN_LAT, BEGIN_LON)]
ggplot(tmp, aes(x = BEGIN_LON, y = BEGIN_LAT)) + bp

tmp
```

So far not so good. What about `EST`?

```{r}
tmp <- DT[EST, .(EVENT_ID, CZ_NAME, STATE, CZ_TIMEZONE, BEGIN_LAT, BEGIN_LON)]

ggplot(tmp, aes(x = BEGIN_LON, y = BEGIN_LAT)) + bp
```

Ah, geez....

I'm afraid to ask about `CST`.

```{r, fig.asp = 0.7}
tmp <- DT[CST, .(EVENT_ID, CZ_NAME, STATE, CZ_TIMEZONE, BEGIN_LAT, BEGIN_LON)]

ggplot(tmp, aes(x = BEGIN_LON, y = BEGIN_LAT)) + bp
```

Wow!

So we cannot rely on the timezone data in these datasets! Luckily for us, we have other options.

We have access to a FIPS dataset that gives us some of the same information we already have in our `details` dataset. But, hopefully, more accurate. Let's look at EST:

```{r}
fips <- fips_dt()

tmp <- fips[TIME_ZONE == "E", .(LAT, LON)]

ggplot(tmp, aes(x = LON, y = LAT)) + bp
```

*This* is what we expect! 

Now, something to note regardings the `fips_dt()`. You may notice we filtered our data.table by `TIME_ZONE == 'E'` for Eastern Time. 

1. `TIME_ZONE` is a two-character field. Most of the values will be one character. But, there are instances where a FIPS code falls along two time zones. How you choose to handle that is up to you. Being that, in general, weather systems move west to east, it may not be a bad idea to choose the easternmost timezone. Something to consider.

2. The values of `TIME_ZONE` are stored in `fips_tz_abbr()`:

```{r}
fips_tz_abbr()
```

## LUDT

Let's work on a sample `details` dataset where `year == 1965` and get our fips dataset:

```{r}
DT <- get_data(1965, type = "details")
Fips <- fips_dt()
```

Let's filter out the variables we need to rebuild date/time variables. We can recall these variables with `datetime_fields()`:

```{r}
datetime_fields()
```

We'll also need `DT$CZ_FIPS` and `DT$STATE_FIPS` so I'll add that to the vector.

```{r}
vars <- c(datetime_fields(), "CZ_FIPS", "STATE_FIPS")

DT <- DT[, vars, with = FALSE]
```

Now, [FIPS county code's](https://en.wikipedia.org/wiki/FIPS_county_code) are five digits. You may notice in `Fips` this is correct. However, in our `DT` data.table it's split into `CZ_FIPS` and `STATE_FIPS`. So, we need to combine them and add them into a lookup table. We do this by sending to `add_fips` a datable consisting only of `EVENT_ID`, `STATE_FIPS` and `CZ_FIPS`. 

I'll create a new data table called `LUDT`. *This is a data table you will want to save*. 

```{r}
LUDT <- add_fips(DT[, .(EVENT_ID, CZ_FIPS, STATE_FIPS)])

str(LUDT)

head(LUDT)
```

We've done two things here: first, created a lookup data table that will hold our clean values. Later, we'll use `LUDT` to add clean time zones and date time variables. 

Second, we now have proper `FIPS` to match up with the time zones. This will come in handy for many other analyses we do later, as well. 

Next, let's see the time zones we're dealing with for `LUDT$FIPS`.

```{r}
setkey(LUDT, FIPS)
setkeyv(Fips, c("FIPS", "TIME_ZONE"))
tmp <- Fips[LUDT, .(EVENT_ID, FIPS, TIME_ZONE), mult = "first"]
unique(tmp$TIME_ZONE)
```

If we look at our documentation for `fips_tz_abbr()` we see the following:

* A   Alaska Standard

* C   Central Standard

* E   Eastern Standard (e = advanced time not observed)

* G   Guam & Marianas

* H   Hawaii-Aleutian Standard

* M   Mountain Standard (m = advanced time not observed)

* P   Pacific Standard

* S   Samoa Standard

* V   Atlantic Standard

So, we have 10 unique time zones (one `NA`) and two in multiple time zones (`MP` and `CM`). Again, the multiples are discretionary. My personal rule with this is to assign the easternmost time zone. So, for `MP` I'll assign `M` or *MST* and for `CM` I'll assign *CST*. 

```{r}
tmp[TIME_ZONE %in% "MP", TIME_ZONE := "M"]
tmp[TIME_ZONE %in% "CM", TIME_ZONE := "C"]
```

Regarding if "advanced time not observed", I'm not sure how to handle this at the moment. For now, I'll ignore it beyond making them upper-case.

```{r}
tmp$TIME_ZONE <- toupper(tmp$TIME_ZONE)
```

And now I'll add `TZ` for the timezone abbreviation.

```{r}
time_zones <- fips_tz_abbr()

tmp[, TZ := as.character(time_zones[TIME_ZONE]), by = TIME_ZONE]
```

Now, we want to add `TZ` to `LUDT`. We can do this by calling `add_tz()`. `add_tz()` demands a data table with the fields `EVENT_ID` and `TZ`. 

```{r}
LUDT <- add_tz(tmp[, .(EVENT_ID, TZ)])
str(LUDT)
```

## Create Datetime Variable

Now that we have some clean time zones we can finish up by applying proper dates and times. What I mean by "proper" is `POSIXct` or `POSIXlt` datetime variables.

I mentioned earlier that the `details` datasets contain 11 variables for dates or times; six of which we put into our `vars` variable (the other five are `BEGIN_YEARMONTH`, `END_YEARMONTH`, `BEGIN_TIME`, `END_TIME` and `CZ_TIMEZONE` which is a complete waste of space). Our `DT` data table still holds those values unedited. 

We'll add two variables to `LUDT`: `BEGIN_DATE` and `END_DATE`. We'll use the `lubridate` package to help and our clean time zones. 

First, we need to merge `DT` with `LUDT` bringing over only `TZ`.

```{r}
tmp <- merge(LUDT[, .(EVENT_ID, TZ)], 
             DT[, c("CZ_FIPS", "STATE_FIPS") := NULL], by = "EVENT_ID")
```

Now, we want to send this data table to `add_datetime()` which will then parse certain values from each of the variables and create our two new variables.

```{r}
LUDT <- add_datetime(tmp)
```

## Verification

Now, we need to make sure we did this correct (why we don't mess with raw data!). I'll get a new `DT` with all variables. I'll also create another `tmp` data table merging `DT` and `LUDT` with relevant variables. Then we'll pull a small sample and look at our results.

```{r}
DT <- get_data(1965, "details")
tmp <- merge(LUDT[, .(EVENT_ID, TZ, BEGIN_DATE, END_DATE)], 
             DT[, .(EVENT_ID, BEGIN_DATE_TIME, END_DATE_TIME, STATE, 
                    CZ_NAME)], by = "EVENT_ID")

set.seed(1)
kable(tmp[, .SD[sample(.N, min(.N, 2))], by = "TZ"])
```

Let's take a look at a couple of these.

`EVENT_ID` 10027049 in Decatur, KS has a time difference of -0500 hours (subtract `BEGIN_DATE_TIME` - our original and local time - from `BEGIN_DATE` - our time in **UTC**). [TimeAndDate.com](http://www.timeanddate.com/time/change/usa/chicago?year=1965) says in 1965, **CDT** lasted from April 25 to October 31. Our date for this record is September, 3. [Central Daylight Time](http://www.timeanddate.com/time/zones/cdt) is five hours behind UTC (-0500). So this checks out. 

We take a quick look at `EVENT_ID` 10049359 and draw the same conclusion; -0500 offset. Daylight savings time was in effect May 27 as noted above, so this is good, as well.

You'll also note using the sources above that *America/Denver* should be -0600 when daylight savings time is in effect. And this is the correct offset for both of our entries. 

Let's look at `EVENT_ID` 10005029 where our date is April 12 which [should be standard time](http://www.timeanddate.com/time/change/usa/new-york?year=1965). [That offset should be -0500](http://www.timeanddate.com/time/zones/est) also (because of the time change). This record checks out as well. 

So, I'm happy with the results at least from what I see. 

## What About Those NULLs in TZ!?!

Yes, those NULLS... notice we don't have a `CZ_NAME` (is `NA`) for those records. 

```{r}
kable(DT[EVENT_ID == 10148785 | EVENT_ID == 10065754, .(CZ_FIPS, 
                                                        STATE_FIPS)])
```

Ahh, no `CZ_FIPS`. Well, we needed it to tie into our `Fips` data table to get the proper time zones. W

This is an easy fix - just update those records in `LUDT` using `force_tz` from the `lubridate` package. You just need to find out the locations and the respective time zones for each. 

At some point I may address these in a seperate data table. At the moment, it's not a priority.
